{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2885908c-bd7a-4936-abdf-b95a6ddf01e4",
   "metadata": {},
   "source": [
    "## 01_NLP: Text Preprocessing Techniques\n",
    "\n",
    "Definition:\n",
    "Text preprocessing in NLP (Natural Language Processing) is the process of cleaning and preparing raw text data for analysis or machine learning models. It improves model accuracy by standardizing text.\n",
    "\n",
    "#### Example: <br>\n",
    "Raw text: \"Hey!!! Visit https://example.com now üòé #exciting\"  <br>\n",
    "After preprocessing: \"hey visit exciting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e56184-6a8b-4467-8717-16d469e2086f",
   "metadata": {},
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "906c7317-34bf-464b-8ca4-3d37ac189b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello World!\"\n",
    "text_lower = text.lower()\n",
    "print(text_lower)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5488f6a8-9acc-47d2-ac51-4f5f08d58edf",
   "metadata": {},
   "source": [
    "### Remove HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4e2b10f-0eb7-48a7-9c05-4944e6c61d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_text = \"<p>Hello <b>World</b></p>\"\n",
    "clean_text = BeautifulSoup(html_text, \"html.parser\").get_text()\n",
    "print(clean_text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d92427-556e-49b0-81d1-47b2d3d42377",
   "metadata": {},
   "source": [
    "### Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "411d9042-36dd-46e0-b22f-513b6a78c7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check this out: \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Check this out: https://example.com\"\n",
    "clean_text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "print(clean_text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066f76bf-900b-42ba-b238-ff824c6f0dbe",
   "metadata": {},
   "source": [
    "### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f6208fb-914b-4989-8b9c-c20650fe333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "text = \"Hello, World!\"\n",
    "clean_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61342ff-8957-4d97-8f58-85e1abb59460",
   "metadata": {},
   "source": [
    "### Chat Word Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c790c3e7-0d21-48e1-9c52-99987648a241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how r you laugh out loud\n"
     ]
    }
   ],
   "source": [
    "chat_dict = {\"u\": \"you\", \"lol\": \"laugh out loud\"}\n",
    "text = \"how r u lol\"\n",
    "text = ' '.join([chat_dict.get(word, word) for word in text.split()])\n",
    "print(text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d38cb4-5156-427b-9a1c-0dffaff91343",
   "metadata": {},
   "source": [
    "### Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddbc31d5-b854-4a75-8034-b3fd842c5291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Using cached textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: nltk>=3.9 in e:\\anaconda\\envs\\tf\\lib\\site-packages (from textblob) (3.9.2)\n",
      "Requirement already satisfied: click in e:\\anaconda\\envs\\tf\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: joblib in e:\\anaconda\\envs\\tf\\lib\\site-packages (from nltk>=3.9->textblob) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\anaconda\\envs\\tf\\lib\\site-packages (from nltk>=3.9->textblob) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in e:\\anaconda\\envs\\tf\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: colorama in e:\\anaconda\\envs\\tf\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Using cached textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.19.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# install TextBlob if missing and download corpora required for correction\n",
    "%pip install textblob\n",
    "!python -m textblob.download_corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0630fe42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have good spelling\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I havv goood speling\"\n",
    "corrected_text = str(TextBlob(text).correct())\n",
    "print(corrected_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c71bcb",
   "metadata": {},
   "source": [
    "### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "495e3681-2a8b-40d1-ab9c-d33238872a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "text = \"This is a sample sentence\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_text = ' '.join([word for word in word_tokenize(text) if word.lower() not in stop_words])\n",
    "print(filtered_text)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ea091-bf6c-4f1b-9c55-65fa0ea35ae6",
   "metadata": {},
   "source": [
    "### Handling Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71541da2-691a-4b49-8be9-fde863c6de84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Using cached emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Using cached emoji-2.15.0-py3-none-any.whl (608 kB)\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install emoji if missing\n",
    "%pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4159201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love NLP \n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "text = \"I love NLP üòç\"\n",
    "clean_text = emoji.replace_emoji(text, replace='')\n",
    "print(clean_text)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184fad20-c45a-402c-b678-0f13bdedbdcf",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d01ca200-7f7c-45fa-ae26-989784d6b59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"I love NLP\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cfca3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af02eeb2-2b0b-4e2f-800a-5c0107a589e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
