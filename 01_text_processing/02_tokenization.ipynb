{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae45a7a-acd9-476c-b639-73c036d13470",
   "metadata": {},
   "source": [
    "## NLP_02 : Tokenization\n",
    "### What this notebook now includes\n",
    "‚úÖ Basic Tokenization <br>\n",
    "‚úÖ Sentence tokenization <br>\n",
    "‚úÖ Word tokenization <br>\n",
    "‚úÖ Whitespace tokenizer <br>\n",
    "‚úÖ Regex-based tokenizer <br>\n",
    "‚úÖ Tweet tokenizer (social media) <br>\n",
    "‚úÖ Punkt tokenizer <br>\n",
    "‚úÖ Treebank tokenizer <br>\n",
    "‚úÖ WordPunctTokenizer <br>\n",
    "‚úÖ Character Tokenization<br>\n",
    "‚úÖ Clear ‚Äúwhen to use which‚Äù guidance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2d3ca8-51fe-4c6b-b4c8-3bce1f01180b",
   "metadata": {},
   "source": [
    "### Basic Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d439a6c-4791-4524-aa8c-15e411a1b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural Language Processing is amazing!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ecf76ab-fad0-4fcb-9f9d-a16656ccbd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'is', 'amazing!']\n"
     ]
    }
   ],
   "source": [
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e64ced0-d3c9-4c22-9788-ec961364dffa",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85a9fce9-43fc-4fb4-986b-3806dabb4cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b908698e-96be-40c4-ab62-d226c4192b77",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "257ff96d-e2f4-4678-bab8-039bf5b8e738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7c2b868-bbfe-414a-9e05-185e5fc451db",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLP is fun. Tokenization is the first step.\"\n",
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48deed02-692e-4230-8504-6064811e90ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP is fun.', 'Tokenization is the first step.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7bc004-b36a-46d1-b9cc-2853d7d6ff54",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "306c4b24-7510-4887-ba82-d725a6560a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61b701b3-9b6c-4cd9-aa3c-42f6d3088e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello world! This is NLP. Tokenization is important.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "129528ea-dc6b-4b64-97b7-3f12882f39fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e24214b9-ad47-49d4-96ab-82597b06f4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!', 'This', 'is', 'NLP', '.', 'Tokenization', 'is', 'important', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311b72e0-b302-4bb3-aaa4-5c6fb31add55",
   "metadata": {},
   "source": [
    "### Whitespace Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1402e1b2-5d48-4f43-aefb-79f629f4e724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '(NLP)',\n",
       " 'enables',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'human',\n",
       " 'language.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "text = \"Natural Language Processing (NLP) enables computers to understand human language.\"\n",
    "\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fc28ee-8587-4b71-be70-d52b5fd575c0",
   "metadata": {},
   "source": [
    "### Regular Expression Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1501f09a-765f-4d0f-bcf2-1be146c762d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', 's', 'see', 'how', 'it', 's', 'working']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "text = \"Let's see how it's working.\"\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d26c5-7a9b-4710-8e21-3c2953874e99",
   "metadata": {},
   "source": [
    "#### Tweet Tokenization (Social Media Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a33a6a0f-d242-40c5-944e-fecd81936b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'NLP', '!', 'üòç', '#AI', '#MachineLearning']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet = \"I love NLP! üòç #AI #MachineLearning\"\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "tokenizer.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29747c5-e3be-44a0-ae56-5eaa2095e193",
   "metadata": {},
   "source": [
    "#### TreebankWordTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62eb7165-1c4f-423f-8909-8187625ff959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', \"'s\", 'see', 'how', 'it', \"'s\", 'working', '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b7c077-252e-4a9a-ab37-2c0cd2061e81",
   "metadata": {},
   "source": [
    "#### WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e524127-9bb8-402d-9243-ddc523b559c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', \"'\", 's', 'see', 'how', 'it', \"'\", 's', 'working', '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(\"Let's see how it's working.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522eaa7c-d3b1-41c4-b821-5a294089b3dc",
   "metadata": {},
   "source": [
    "### Character Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755689b5-bf0a-4d63-8161-4a7cfabfe6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Tokens:\n",
      "['L', 'e', 't', \"'\", 's', ' ', 's', 'e', 'e', ' ', 'h', 'o', 'w', ' ', 'i', 't', \"'\", 's', ' ', 'w', 'o', 'r', 'k', 'i', 'n', 'g', '.']\n"
     ]
    }
   ],
   "source": [
    "# Character-level tokenization splits text into characters.\n",
    "\n",
    "char_tokens = list(text)\n",
    "print(\"Character Tokens:\")\n",
    "print(char_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163c6b43-5532-4716-9db5-11d7e7b5ecde",
   "metadata": {},
   "source": [
    "### Punkt Tokenizer (Rule-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a69104c3-8e9b-4482-a6bc-c530060a9d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Let's see how it's working.\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "punkt = PunktSentenceTokenizer()\n",
    "punkt.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c344909e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
