{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1b473c-5818-4b57-8f6a-e17d74bf1b7f",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# NLP_07 : Word2Vec — CBOW & Skip-Gram\n",
    "\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "### What is Word2Vec?\n",
    "\n",
    "**Word2Vec** is a word embedding technique that represents words as **dense, low-dimensional vectors**.  \n",
    "Unlike BoW or TF-IDF, Word2Vec captures **semantic meaning** and **contextual similarity** between words.\n",
    "\n",
    "Words with similar meanings tend to have **similar vector representations**.\n",
    "\n",
    "### Why Word2Vec?\n",
    "\n",
    "Limitations of BoW and TF-IDF:\n",
    "- High-dimensional and sparse vectors\n",
    "- No semantic understanding\n",
    "- No context awareness\n",
    "\n",
    "Word2Vec solves these problems by:\n",
    "- Learning dense vectors\n",
    "- Capturing semantic relationships\n",
    "- Preserving contextual information\n",
    "\n",
    "### Two Architectures of Word2Vec\n",
    "\n",
    "Word2Vec has **two training models**:\n",
    "\n",
    "1. **CBOW (Continuous Bag of Words)**\n",
    "2. **Skip-Gram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd453e0c-cbca-420c-bf1d-2db63edac3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df050fb6-4f67-4762-b2ac-6a44ab8f6765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the text data\n",
    "story = []\n",
    "for filename in os.listdir('data'):\n",
    "    f = open(os.path.join('data', filename))\n",
    "    corpus = f.read()\n",
    "    raw_sent = sent_tokenize(corpus)\n",
    "    for sent in raw_sent:\n",
    "        story.append(simple_preprocess(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d169801e-858e-473a-85ca-2679a8468988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 145020\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of sentences: {len(story)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c509ea-74ca-4c91-ade4-7e6ceb69878d",
   "metadata": {},
   "source": [
    "#### Train a CBOW model and extract word vectors and Train a Skip-Gram model and compare the results with CBOW.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edefb7c-8fc6-4e9d-b363-cba24cbb32f4",
   "metadata": {},
   "source": [
    "## CBOW (Continuous Bag of Words)\n",
    "\n",
    "### Definition\n",
    "**CBOW (Continuous Bag of Words)** is a Word2Vec model that predicts a **target word** based on its **surrounding context words**.It learns word embeddings by using the **neighboring words** within a fixed window to estimate the missing word.In CBOW, **word order is ignored**, and only the presence of context words is considered.\n",
    "\n",
    "### Example\n",
    "\n",
    "Sentence: I love natural language processing <br>\n",
    "If the **target word** is: language <br>\n",
    "\n",
    "And the **context window size = 2**, then the **context words** are: \"love\", \"natural\", \"processing\"<br>\n",
    "CBOW learns the mapping:\n",
    "Context Words → Target Word <br>\n",
    "[\"love\", \"natural\", \"processing\"] → \"language\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff766dd-ed87-4219-ae8f-c444e9199224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a CBOW model\n",
    "cbow_model = Word2Vec(\n",
    "    sentences=story,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=0  # CBOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1033a4a2-23ae-440e-a3ec-6112a497376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary and train\n",
    "cbow_model.build_vocab(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dcb50ea-fa15-4117-9752-a729bd34e159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6570205, 8628190)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.train(story, total_examples=cbow_model.corpus_count, epochs=cbow_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db922bf9-7dbb-48b7-86de-4909011a3df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and analyze CBOW vectors\n",
    "cbow_vector = cbow_model.wv['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9ba431c-c6c3-4bf8-9035-9acf11a086a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW vector shape for 'king': (100,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"CBOW vector shape for 'king': {cbow_vector.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14ce3d1a-5e81-4076-8970-41cbb5c96a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_vectors = cbow_model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bfc76ae-03bf-4c0f-819d-da34bfa402ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_vocab = list(cbow_model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bff70b94-5f63-40be-8a7a-1111ad3987fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CBOW Model Results:\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCBOW Model Results:\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90fd2755-7442-4486-ae77-b2693a071a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Similar to king:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('realm', 0.6187288165092468),\n",
       " ('baratheon', 0.6147810816764832),\n",
       " ('aegon', 0.49529320001602173),\n",
       " ('robert', 0.4896616041660309),\n",
       " ('rebellion', 0.48941537737846375),\n",
       " ('throne', 0.4880048334598541),\n",
       " ('heir', 0.48798668384552),\n",
       " ('lannisters', 0.47327372431755066),\n",
       " ('war', 0.4626232385635376),\n",
       " ('renly', 0.4624309241771698)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"CBOW Similar to king:\")\n",
    "cbow_model.wv.most_similar(\"king\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "029f48cb-dd9d-4b89-8449-fd7f20007afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Similar to queen:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('princess', 0.5955092906951904),\n",
       " ('kingslayer', 0.5950101613998413),\n",
       " ('joffrey', 0.5702580809593201),\n",
       " ('sister', 0.5633024573326111),\n",
       " ('margaery', 0.5477557182312012),\n",
       " ('myrcella', 0.5228344202041626),\n",
       " ('beloved', 0.5170937180519104),\n",
       " ('marriage', 0.5148559808731079),\n",
       " ('cersei', 0.5106081366539001),\n",
       " ('approach', 0.5100813508033752)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"CBOW Similar to queen:\")\n",
    "cbow_model.wv.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a875acad-baea-459a-a9f8-7b65b732b381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.5519255995750427),\n",
       " ('princess', 0.5244705677032471),\n",
       " ('baratheon', 0.5238691568374634),\n",
       " ('mother', 0.44155389070510864),\n",
       " ('aegon', 0.43635252118110657),\n",
       " ('murdered', 0.4243878722190857),\n",
       " ('realm', 0.41330569982528687),\n",
       " ('margaery', 0.4110272228717804),\n",
       " ('court', 0.40610864758491516),\n",
       " ('shame', 0.40155017375946045)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.most_similar(positive=['king','woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce29f625-35eb-420b-89bf-cbd5bfd96500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word that doesn't match in ['cersei', 'jaime', 'broom', 'tyrion']:\n",
      "broom\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nWord that doesn't match in ['cersei', 'jaime', 'broom', 'tyrion']:\")\n",
    "print(cbow_model.wv.doesnt_match(['cersei', 'jaime', 'broom', 'tyrion']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "721b4685-e274-4d74-967b-ce31b271f496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8337543"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.similarity('arya','sansa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcb2fa38-2f32-40bb-a796-828b73fb829b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6142799"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.similarity('cersei','sansa')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbe6488-0f97-47ba-a163-bc27e9d203f7",
   "metadata": {},
   "source": [
    "### Characteristics\n",
    "- Faster training\n",
    "- Works well with large datasets\n",
    "- Better for frequent words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4902560-7b14-439a-bf61-5b416b5bd482",
   "metadata": {},
   "source": [
    "### Skip-Gram (Word2Vec Model)\n",
    "\n",
    "### Definition\n",
    "**Skip-Gram** is a Word2Vec model that predicts the **surrounding context words** given a **target word**.The goal of Skip-Gram is to learn word embeddings that are good at representing **semantic and contextual relationships**, especially for **rare words**.\n",
    "\n",
    "### Learning Objective\n",
    "### Example\n",
    "\n",
    "Sentence: I love natural language processing <br>\n",
    "If the **target word** is: language <br>\n",
    "And the **context window size = 2**, the **context words** are: [\"love\", \"natural\", \"processing\"] <br>\n",
    "Skip-Gram learns the mappings: <br>\n",
    "\"language\" → \"love\" <br>\n",
    "\"language\" → \"natural\" <br>\n",
    "\"language\" → \"processing\" <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16556ac9-f24c-4964-ad1b-fb6924da73a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Skip-Gram model\n",
    "skipgram_model = Word2Vec(\n",
    "    sentences=story,\n",
    "    vector_size=100,\n",
    "    window=5,  # Changed from 2 to 5 for consistency with CBOW\n",
    "    min_count=2,\n",
    "    sg=1,    # Skip-gram\n",
    "    workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb2216e1-9904-49be-bed4-03f5b2dc6b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6570360, 8628190)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build vocabulary and train Skip-Gram\n",
    "skipgram_model.build_vocab(story)\n",
    "skipgram_model.train(story, total_examples=skipgram_model.corpus_count, epochs=skipgram_model.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e80de791-6087-4701-b1a3-9d0b1886c7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Skip-Gram Model Results:\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Skip-Gram Model Results:\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0d0b90c-9011-4bb6-8a14-8fcc6215feb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similar to 'king':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('landing', 0.7086208462715149),\n",
       " ('ii', 0.6590932607650757),\n",
       " ('jaehaerys', 0.6577804684638977),\n",
       " ('robert', 0.6571977734565735),\n",
       " ('pretender', 0.651711106300354),\n",
       " ('baratheon', 0.6515921950340271),\n",
       " ('tommen', 0.6410520076751709),\n",
       " ('joffrey', 0.6385347247123718),\n",
       " ('condemned', 0.633653998374939),\n",
       " ('proclaim', 0.6309674382209778)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"\\nSimilar to 'king':\")\n",
    "skipgram_model.wv.most_similar(\"king\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de77501a-2363-4a6c-9b35-91e7aaab6748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similar to 'queen':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('cersei', 0.7103806734085083),\n",
       " ('margaery', 0.6944690942764282),\n",
       " ('regent', 0.6815361976623535),\n",
       " ('selyse', 0.6643837094306946),\n",
       " ('unburnt', 0.6469699740409851),\n",
       " ('hizdahr', 0.6462743878364563),\n",
       " ('niece', 0.624958872795105),\n",
       " ('joffrey', 0.6215522289276123),\n",
       " ('taena', 0.6164122819900513),\n",
       " ('sister', 0.6102229356765747)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"\\nSimilar to 'queen':\")\n",
    "skipgram_model.wv.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "851bfdda-bfc5-461d-86c7-c282bf91ea27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Comparison Summary:\n",
      "==================================================\n",
      "Vocabulary size (CBOW): 17453\n",
      "Vocabulary size (Skip-Gram): 17453\n",
      "CBOW 'king' vector dimension: (100,)\n",
      "Skip-Gram 'king' vector dimension: (100,)\n"
     ]
    }
   ],
   "source": [
    "# Compare results\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Comparison Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Vocabulary size (CBOW): {len(cbow_vocab)}\")\n",
    "print(f\"Vocabulary size (Skip-Gram): {len(skipgram_model.wv.index_to_key)}\")\n",
    "print(f\"CBOW 'king' vector dimension: {cbow_vector.shape}\")\n",
    "print(f\"Skip-Gram 'king' vector dimension: {skipgram_model.wv['king'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8223190-722a-44cb-bc15-e93589682813",
   "metadata": {},
   "source": [
    "### Characteristics\n",
    "- Slower than CBOW\n",
    "- Works well with small datasets\n",
    "- Better for rare words\n",
    "\n",
    "## CBOW vs Skip-Gram (Conceptual Comparison)\n",
    "\n",
    "| Feature | CBOW | Skip-Gram |\n",
    "|------|------|-----------|\n",
    "| Prediction direction | Context → Target | Target → Context |\n",
    "| Training speed | Faster | Slower |\n",
    "| Performance on rare words | Lower | Better |\n",
    "| Dataset size suitability | Large datasets | Small datasets |\n",
    "| Computational cost | Lower | Higher |\n",
    "\n",
    "## Word2Vec Training Mechanism (High-Level)\n",
    "\n",
    "Both CBOW and Skip-Gram use:\n",
    "- A shallow neural network\n",
    "- One hidden layer\n",
    "- Backpropagation\n",
    "\n",
    "To improve efficiency, they use:\n",
    "- **Negative Sampling**\n",
    "- **Hierarchical Softmax**\n",
    "  \n",
    "## When to Use CBOW vs Skip-Gram?\n",
    "\n",
    "### Use CBOW when:\n",
    "- Dataset is large\n",
    "- Speed is important\n",
    "- Focus is on frequent words\n",
    "\n",
    "### Use Skip-Gram when:\n",
    "- Dataset is small\n",
    "- Rare words are important\n",
    "- Higher accuracy is required\n",
    "  \n",
    "## Summary\n",
    "\n",
    "- Word2Vec converts words into dense vectors\n",
    "- CBOW predicts target word from context\n",
    "- Skip-Gram predicts context words from target\n",
    "- Both capture semantic relationships effectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ac19c0-435a-4e3c-854a-f2341ff62782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cde84669-8709-4f5f-9356-ddbdb03e7605",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right;\">\n",
    "    <b>Author:</b> Monower Hossen <br>\n",
    "    <b>Date:</b> January 7, 2026\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
